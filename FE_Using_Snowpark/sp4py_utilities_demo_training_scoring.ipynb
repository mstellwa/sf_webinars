{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5729cca3-231f-44b2-a44b-69c23a841081",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0319e-3237-437a-a19b-83c3e5411a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import snowflake.snowpark\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "import joblib\n",
    "import io\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dc499-f14b-4800-9c5e-25c40bcd1a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the preprocessing library from https://github.com/Snowflake-Labs/snowpark-python-demos/tree/main/sp4py_utilities\n",
    "import preprocessing as pp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f41d1-4a3d-43b1-9814-b2057e0896e8",
   "metadata": {},
   "source": [
    "Connect to Snowflake\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68234ce-6537-4f9e-93cd-66f29e41fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(\"Current role: \" + session.get_current_role() + \", Current schema: \" + session.get_fully_qualified_current_schema() + \", Current WH: \" + session.get_current_warehouse())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983ed0e4-c725-4d02-a683-0773630699fc",
   "metadata": {},
   "source": [
    "Function that saves a object as a joblib file on a Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110abc99-5c0b-49b4-b5d6-2eed76558eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function used to save a object to a Snowflake stage, used within the training Stored procedure\n",
    "def save_file_to_stage(snf_session, obj, file_name, stage_path):\n",
    "    file_path = stage_path + file_name\n",
    "    input_stream = io.BytesIO()\n",
    "    input_stream.name = file_name\n",
    "    joblib.dump(obj, input_stream)\n",
    "    snf_session.file.put_stream(input_stream, file_path, auto_compress=False, overwrite=True)\n",
    "    \n",
    "    return file_path + '/' + file_name # a workaround\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1665690d-f62c-4b39-85e9-7d9facdc10f6",
   "metadata": {},
   "source": [
    "Function that scales numeric columns using a Robust Scaler and encode categorical columns using One-Hot encoding.<br>\n",
    "It saves the fitted scaler and encoder to a Snowflake stage.<br>\n",
    "Used by the traning stored procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c3038-b155-4415-a402-e040f3ec5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(snf_session, df, num_cols, cat_cols, file_location):\n",
    "    \n",
    "    # Numeric columns\n",
    "    # Generate names for the output columns\n",
    "    scaler_output_cols = [col + \"_SCALED\" for col in num_cols]\n",
    "    rs = pp.RobustScaler(input_cols=num_cols, output_cols=scaler_output_cols)\n",
    "    # Fit and transform\n",
    "    df_scaled = rs.fit_transform(df)\n",
    "    # Drop the input columns from \n",
    "    df_scaled = df_scaled.drop(num_cols)\n",
    "    # Store the fitted scaler in a Snowflake stage\n",
    "    scaler_path = save_file_to_stage(snf_session, rs, 'my_rs_scaler.joblib', file_location)\n",
    "\n",
    "    # Categorical columns using Ordinal Encoder\n",
    "    oe = pp.OneHotEncoder(input_cols=cat_cols)\n",
    "    \n",
    "    # Using the transformed dataframe since we can not extend a dataframe with another (we can stack using UNION)\n",
    "    df_return = oe.fit_transform(df_scaled)\n",
    "    \n",
    "    # Store the fitted encoder in a Snowflake stage\n",
    "    encoder_path = save_file_to_stage(snf_session, oe, 'my_oe_encoder.joblib', file_location)\n",
    "    \n",
    "    # Returned the transformed dataframe\n",
    "    return (scaler_path, encoder_path, df_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaa6642-51e1-43c0-a058-f9bb327d413d",
   "metadata": {},
   "source": [
    "Function that deploy a trained model as a Vectorized UDF in Snowflake, used by the training stored procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34bffd-877f-457c-abb4-4b15c91daaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_udf(snf_session, udf_name, model, input_cols, stage_loc):\n",
    "    # Deploy a Batch API UDF\n",
    "    @F.udf(name=udf_name, is_permanent=True, stage_location=stage_loc, packages=['pandas', 'scikit-learn'], replace=True, session=snf_session)\n",
    "    def preidict_intl_plan(ds: T.PandasSeries[dict]) -> T.PandasSeries[float]:\n",
    "        df = pd.io.json.json_normalize(ds)[input_cols]\n",
    "        prediction = model.predict(df)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a3158-9a0f-4b43-b007-b216bd6e2a60",
   "metadata": {},
   "source": [
    "Function that will transform the input data using scaler and encoder, save the fitted transformers to a Snowflake\n",
    "stage and train a model that is deployed as a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae62d2f-c91c-483b-b612-522ce3dca7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(snf_session: snowflake.snowpark.Session, params: dict) -> dict:\n",
    "    # Use the table with the name in train_data_table\n",
    "    df_cat_cols = params['cat_cols']\n",
    "    df_num_cols = params['num_cols']\n",
    "    df_target_col = params['target_col']\n",
    "        \n",
    "    df = snf_session.table(params['train_input_table']).select(*df_cat_cols, *df_num_cols, df_target_col)\n",
    "    # Imputation\n",
    "    # Numeric columns, replace missing values with the mean value\n",
    "    imputation = {}\n",
    "    for num_col in df_num_cols:\n",
    "        imputation[num_col] = df.select(F.mean(num_col)).collect()[0][0]\n",
    "    \n",
    "    # Categorical columns, replace values with the most frequent\n",
    "    for cat_col in df_cat_cols:\n",
    "        imputation[cat_col] = df.select(F.mode(cat_col)).collect()[0][0]\n",
    "    \n",
    "    df = df.fillna(imputation)\n",
    "        \n",
    "    # Prepare the data and store the fitted scaler and encoder to stage\n",
    "    num_scaler_path, cat_encoder_path, df_prepared = data_prep(snf_session, df, df_num_cols, df_cat_cols, params['transfomers_location'])\n",
    "    \n",
    "    # Save the prepared data into train_ouput_table\n",
    "    df_prepared.write.mode(\"overwrite\").save_as_table(params['train_ouput_table'])\n",
    "    X_cols = df_prepared.columns\n",
    "    X_cols.remove(df_target_col)\n",
    "    \n",
    "    # Train the classifier model, pull back data into a Pandas dataframe\n",
    "    pd_df = df_prepared.to_pandas()\n",
    "    \n",
    "    X = pd_df[X_cols]\n",
    "    Y = pd_df[df_target_col]\n",
    "    clf = RandomForestClassifier(n_estimators=10)\n",
    "    model = clf.fit(X, Y)\n",
    "    \n",
    "    # Deploy the model to Snowflake, the model is stored as part of the UDF definition ie not stored on a stage\n",
    "    create_udf(snf_session, params['model_udf_name'], model, X_cols, params['model_udf_stage'])\n",
    "    # Return the path to the saved encoder and scaler and the impute values\n",
    "    return_dict = {\"num_scaler_path\": num_scaler_path, \"cat_encoder_path\": cat_encoder_path, \"imputer\":imputation}\n",
    "    return return_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec9d87-0c81-4d16-b33c-c9ab005bb277",
   "metadata": {},
   "source": [
    "Create an instance of Stored Procedure using the sproc() function, the Store procedure is using the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5c1c5-a22d-463e-b1de-4dee0e218442",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "train_model_sp = F.sproc(train, name=\"prep_train_titanic\", is_permanent=True, session = session,stage_location='udf_stage', replace=True, \n",
    "                         packages=['snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib', 'cachetools'], imports=['preprocessing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb411e38-953f-4f55-bbd5-83771d4cf395",
   "metadata": {},
   "source": [
    "Set the parameters need for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a3c0d-c2be-4ada-b687-97da945c8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_param = {\n",
    "    \"train_input_table\": \"titanic\",\n",
    "    \"cat_cols\": [\"EMBARKED\", \"SEX\", \"PCLASS\"],\n",
    "    \"num_cols\": [\"AGE\", \"FARE\"],\n",
    "    \"target_col\": \"SURVIVED\",\n",
    "    \"train_ouput_table\" : \"titanic_preped_train\",\n",
    "    \"transfomers_location\": \"@udf_stage/titanic_train/\",\n",
    "    \"model_udf_name\": \"predict_survival\",\n",
    "    \"model_udf_stage\": \"udf_stage\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e6370-f2cf-457b-ba23-7580422fc369",
   "metadata": {},
   "source": [
    "Run the training in Snowflake using the Stored Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8168cfcc-ec59-4930-9c4f-734458ed6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dict = train_model_sp(train_param)\n",
    "transformers = json.loads(ret_dict)\n",
    "transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb608f-e88b-4767-9631-ba721063342b",
   "metadata": {},
   "source": [
    "We can decide to either run the csoring from our local enviroment, by using Snowpark for Python everything is pushed down to Snowflake, or deploy the scoring function as a Python Stored Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210b7c4-18a3-47a8-bc59-6c4c51fdfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to transform input_data (list of list) within Snowflake\n",
    "# This function would be executed outside Snowflake\n",
    "def load_file_from_stage(snf_session: snowflake.snowpark.Session, object_path): \n",
    "    output_stream = snf_session.file.get_stream(object_path)\n",
    "    obj = joblib.load(output_stream)\n",
    "    return obj\n",
    "\n",
    "def score(snf_session: snowflake.snowpark.Session, params: dict) -> str:\n",
    "    # Convert the input data to a Snowpark dataframe\n",
    "    df_input = snf_session.table(params['score_input_table'])[params[\"score_cols\"]]\n",
    "    \n",
    "    # Add the imputation\n",
    "    df_imputed = df_input.fillna(params[\"transformers\"]['imputer'])\n",
    "    # Load the scaler and scale the numeric columns\n",
    "    loaded_rs = load_file_from_stage(snf_session, params[\"transformers\"]['num_scaler_path'])\n",
    "    df_scaled = loaded_rs.transform(df_imputed)\n",
    "    \n",
    "    # load the encoder and encode categorical columns\n",
    "    loaded_oe = load_file_from_stage(snf_session, params[\"transformers\"]['cat_encoder_path'])\n",
    "    df_transformed = loaded_oe.transform(df_scaled)\n",
    "\n",
    "    # Generate a \n",
    "    key_vals = []\n",
    "    for col in loaded_rs.output_cols:\n",
    "        key_vals.extend([F.lit(col.upper()), F.col(col)])\n",
    "    \n",
    "    for in_col in loaded_oe.output_cols:\n",
    "        for col in loaded_oe.output_cols[in_col]:\n",
    "            key_vals.extend([F.lit(col.upper()), F.col(col)])\n",
    "    \n",
    "    # Score using the deployed UDF and using object_construct to create a dict that will be part of a Pandas series\n",
    "    # as input to our UDF. The select is because the scaler will keep the input columns...\n",
    "    df_scored = df_transformed.with_column(\"PREDICTED\", F.call_function(params[\"model_udf_name\"],  F.object_construct(*key_vals)))\n",
    "    \n",
    "    df_scored.write.mode(\"overwrite\").save_as_table(params['score_output_table'])\n",
    "    # Return the dataframe with scores, could use save_as_table to store the result in Snowflake instead\n",
    "    return f\"Saved scored resutl in: {params['score_output_table']}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6a5ae-cbba-4655-9581-9bfb6110a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "score_titanic_sp = F.sproc(score, name=\"score_titanic\", is_permanent=True, session = session,stage_location='udf_stage', replace=True, \n",
    "                         packages=['snowflake-snowpark-python', 'scikit-learn', 'pandas', 'numpy', 'joblib'], imports=['preprocessing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f181eff5-c3b5-4158-a38a-41b7d496d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_param = {\n",
    "    \"score_input_table\": \"titanic\",\n",
    "    \"score_cols\": [\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\", \"SURVIVED\"],\n",
    "    \"score_output_table\": \"titanic_scored\",\n",
    "    \"transformers\" : transformers,\n",
    "    \"model_udf_name\": \"predict_survival\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309bd29-5313-453a-93d2-e32762662f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_titanic_sp(session, score_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b5ad9-5577-467c-8c9d-2419a550619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"titanic_scored\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a714496-4f6f-412f-9649-782bcc9cc906",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87647b-2232-4c30-bc13-db00c33daa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
